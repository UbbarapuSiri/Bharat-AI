#!/usr/bin/env python3
"""
Sample Analysis Runner for Food Health Rating App

This script demonstrates the complete analysis pipeline by processing
three sample products and generating all required outputs.
"""

import json
import os
from datetime import datetime
from pathlib import Path

# Import our app components
from app import (
    ProductData, HealthScore, IngredientNormalizer, 
    NutrientNormalizer, HealthScorer, ProductDatabase
)

def create_sample_products():
    """Create sample products for testing different score ranges"""
    
    # Sample 1: High-scoring product (Oatmeal with added nutrients)
    sample_1_nutrients = {
        'calories': '150',
        'total_fat': '3g',
        'saturated_fat': '0.5g',
        'sodium': '0mg',
        'total_carbohydrates': '27g',
        'dietary_fiber': '8g',
        'total_sugars': '1g',
        'protein': '5g'
    }
    
    sample_1_ingredients = "whole grain oats, wheat bran, flax seeds, almonds, natural vanilla flavor"
    
    normalized_nutrients_1 = NutrientNormalizer.normalize_nutrients(sample_1_nutrients, 40)
    normalized_ingredients_1 = IngredientNormalizer.normalize_ingredient_list(sample_1_ingredients)
    
    product_1 = ProductData(
        barcode="123456789001",
        name="Organic Steel Cut Oats with Flax",
        brand="Healthy Choice",
        ingredients=normalized_ingredients_1,
        nutrients=normalized_nutrients_1,
        serving_size_g=40.0,
        categories=["breakfast", "cereal"]
    )
    
    # Sample 2: Medium-scoring product (Crackers)  
    sample_2_nutrients = {
        'calories': '120',
        'total_fat': '4g',
        'saturated_fat': '1g',
        'sodium': '230mg',
        'total_carbohydrates': '19g',
        'dietary_fiber': '3g',
        'total_sugars': '2g',
        'protein': '3g'
    }
    
    sample_2_ingredients = "whole wheat flour, sunflower oil, sea salt, yeast, natural flavor"
    
    normalized_nutrients_2 = NutrientNormalizer.normalize_nutrients(sample_2_nutrients, 30)
    normalized_ingredients_2 = IngredientNormalizer.normalize_ingredient_list(sample_2_ingredients)
    
    product_2 = ProductData(
        barcode="123456789002", 
        name="Whole Wheat Crackers",
        brand="Natural Foods Co",
        ingredients=normalized_ingredients_2,
        nutrients=normalized_nutrients_2,
        serving_size_g=30.0,
        categories=["snack"]
    )
    
    # Sample 3: Low-scoring product (Processed snack)
    sample_3_nutrients = {
        'calories': '160',
        'total_fat': '10g',
        'saturated_fat': '4g',
        'sodium': '420mg',
        'total_carbohydrates': '15g',
        'dietary_fiber': '1g',
        'total_sugars': '12g',
        'protein': '2g'
    }
    
    sample_3_ingredients = "enriched wheat flour, high fructose corn syrup, palm oil, salt, artificial flavors, red dye 40, bht, sodium nitrite"
    
    normalized_nutrients_3 = NutrientNormalizer.normalize_nutrients(sample_3_nutrients, 28)
    normalized_ingredients_3 = IngredientNormalizer.normalize_ingredient_list(sample_3_ingredients)
    
    product_3 = ProductData(
        barcode="123456789003",
        name="Cheesy Snack Crackers", 
        brand="Processed Foods Inc",
        ingredients=normalized_ingredients_3,
        nutrients=normalized_nutrients_3,
        serving_size_g=28.0,
        categories=["snack"]
    )
    
    return [product_1, product_2, product_3]

def run_complete_analysis():
    """Execute complete analysis pipeline and generate all outputs"""
    
    print("üöÄ Starting Food Health Rating App Sample Analysis")
    print("=" * 60)
    
    # Initialize components
    scorer = HealthScorer()
    db = ProductDatabase("products.db")
    
    # Create output directory
    output_dir = Path("sample_outputs")
    output_dir.mkdir(exist_ok=True)
    screenshots_dir = output_dir / "screenshots" 
    screenshots_dir.mkdir(exist_ok=True)
    
    # Generate sample products
    print("\nüì¶ Creating sample products...")
    products = create_sample_products()
    
    # Analyze each product
    results = []
    for i, product in enumerate(products, 1):
        print(f"\nüîç Analyzing Product {i}: {product.name}")
        print("-" * 40)
        
        # Generate score
        score = scorer.score_product(product)
        
        # Save to database
        data_hash = db.save_product(product, score)
        
        # Create analysis trace
        trace = {
            "timestamp": datetime.now().isoformat(),
            "input_stage": {
                "raw_barcode": product.barcode,
                "raw_name": product.name,
                "raw_ingredients_count": len(product.ingredients),
                "raw_nutrients_count": len(product.nutrients)
            },
            "normalization_stage": {
                "normalized_ingredients": product.ingredients,
                "normalized_nutrients": {k: {
                    "value": v.value,
                    "unit": v.unit, 
                    "per_100g": v.per_100g
                } for k, v in product.nutrients.items()},
                "serving_size_g": product.serving_size_g
            },
            "scoring_stage": {
                "base_score": 50,
                "score_deltas": [{"factor": d.factor, "delta": d.score_delta} for d in score.drivers],
                "final_score": score.overall_score,
                "confidence": score.confidence
            },
            "explanation_stage": {
                "drivers": [{"factor": d.factor, "impact": d.impact, "explanation": d.explanation} for d in score.drivers],
                "warnings": score.warnings,
                "evidence_sources": score.evidence_sources
            }
        }
        
        # Create output bundle
        output_bundle = {
            "product_data": {
                "barcode": product.barcode,
                "name": product.name, 
                "brand": product.brand,
                "ingredients": product.ingredients,
                "nutrients": {k: {
                    "name": v.name,
                    "value": v.value,
                    "unit": v.unit,
                    "per_100g": v.per_100g
                } for k, v in product.nutrients.items()},
                "serving_size_g": product.serving_size_g,
                "categories": product.categories
            },
            "health_score": {
                "overall_score": score.overall_score,
                "band": score.band,
                "drivers": [{
                    "factor": d.factor,
                    "impact": d.impact, 
                    "score_delta": d.score_delta,
                    "explanation": d.explanation,
                    "source": d.source
                } for d in score.drivers],
                "evidence_sources": score.evidence_sources,
                "confidence": score.confidence,
                "warnings": score.warnings
            },
            "analysis_trace": trace,
            "data_hash": data_hash
        }
        
        # Save output bundle
        output_file = output_dir / f"product_{i}.json"
        with open(output_file, 'w') as f:
            json.dump(output_bundle, f, indent=2)
        
        results.append(output_bundle)
        
        # Print summary
        print(f"‚úÖ Score: {score.overall_score}/100 (Grade {score.band})")
        print(f"üìä Confidence: {score.confidence}")
        print(f"üéØ Key Drivers: {len(score.drivers)}")
        print(f"‚ö†Ô∏è  Warnings: {len(score.warnings)}")
        print(f"üíæ Saved to: {output_file}")
    
    # Generate sources snapshot
    print("\nüìö Creating sources snapshot...")
    sources_snapshot = {
        "timestamp": datetime.now().isoformat(),
        "sources": [
            {
                "title": "FDA Nutrition Facts Label Guidelines",
                "year": "2016",
                "url": "https://www.fda.gov/food/nutrition-facts-label",
                "access_date": datetime.now().strftime("%Y-%m-%d"),
                "license": "Public Domain"
            },
            {
                "title": "WHO Global Strategy on Diet, Physical Activity and Health", 
                "year": "2004",
                "url": "https://www.who.int/publications/i/item/9241592222",
                "access_date": datetime.now().strftime("%Y-%m-%d"),
                "license": "Creative Commons"
            },
            {
                "title": "Dietary Guidelines for Americans 2020-2025",
                "year": "2020",
                "url": "https://www.dietaryguidelines.gov/",
                "access_date": datetime.now().strftime("%Y-%m-%d"),
                "license": "Public Domain"
            },
            {
                "title": "European Food Safety Authority Scientific Opinions",
                "year": "Various",
                "url": "https://www.efsa.europa.eu/en/efsajournal",
                "access_date": datetime.now().strftime("%Y-%m-%d"), 
                "license": "Creative Commons"
            },
            {
                "title": "American Heart Association Dietary Guidelines",
                "year": "2021",
                "url": "https://www.heart.org/en/healthy-living/healthy-eating",
                "access_date": datetime.now().strftime("%Y-%m-%d"),
                "license": "All Rights Reserved"
            },
            {
                "title": "Harvard T.H. Chan School of Public Health Nutrition Source",
                "year": "Ongoing",
                "url": "https://www.hsph.harvard.edu/nutritionsource/",
                "access_date": datetime.now().strftime("%Y-%m-%d"),
                "license": "Academic Use"
            }
        ],
        "methodology_notes": {
            "scoring_algorithm": "Evidence-based nutrient thresholds with ingredient classification",
            "confidence_calculation": "Weighted by data completeness and source reliability",
            "bias_mitigation": "Multiple authoritative sources, transparent calculations",
            "limitations": "General population guidelines, individual variation not accounted"
        }
    }
    
    with open("sources_snapshot.json", 'w') as f:
        json.dump(sources_snapshot, f, indent=2)
    
    # Generate run log summary
    print("\nüìÑ Generating run log summary...")
    log_summary = {
        "run_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "timestamp": datetime.now().isoformat(),
        "products_analyzed": len(products),
        "database_records": len(results),
        "output_files_created": [
            "sample_outputs/product_1.json",
            "sample_outputs/product_2.json", 
            "sample_outputs/product_3.json",
            "sources_snapshot.json"
        ],
        "score_distribution": {
            "A_grade": len([r for r in results if r["health_score"]["band"] == "A"]),
            "B_grade": len([r for r in results if r["health_score"]["band"] == "B"]),
            "C_grade": len([r for r in results if r["health_score"]["band"] == "C"]),
            "D_grade": len([r for r in results if r["health_score"]["band"] == "D"]),
            "E_grade": len([r for r in results if r["health_score"]["band"] == "E"])
        },
        "average_score": sum([r["health_score"]["overall_score"] for r in results]) / len(results),
        "confidence_levels": {
            "high": len([r for r in results if r["health_score"]["confidence"] == "high"]),
            "medium": len([r for r in results if r["health_score"]["confidence"] == "medium"]),
            "low": len([r for r in results if r["health_score"]["confidence"] == "low"])
        }
    }
    
    with open(output_dir / "run_summary.json", 'w') as f:
        json.dump(log_summary, f, indent=2)
    
    # Create README for sample outputs
    readme_content = f"""# Sample Analysis Outputs

Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Files in this directory:

### Product Analysis Results
- `product_1.json` - {products[0].name} (Expected: High score)
- `product_2.json` - {products[1].name} (Expected: Medium score)  
- `product_3.json` - {products[2].name} (Expected: Low score)

### Summary Files
- `run_summary.json` - Analysis run statistics and distribution
- `../sources_snapshot.json` - Reference sources with versions and access dates

### Screenshots (to be captured manually)
- `screenshots/main_interface.png` - Main app interface
- `screenshots/analysis_result.png` - Sample analysis result
- `screenshots/evidence_panel.png` - Evidence and sources panel
- `screenshots/history_browser.png` - Product history browser

## Analysis Summary

**Products Analyzed**: {len(products)}
**Average Score**: {log_summary['average_score']:.1f}/100
**Score Distribution**: 
- A Grade: {log_summary['score_distribution']['A_grade']} products
- B Grade: {log_summary['score_distribution']['B_grade']} products  
- C Grade: {log_summary['score_distribution']['C_grade']} products
- D Grade: {log_summary['score_distribution']['D_grade']} products
- E Grade: {log_summary['score_distribution']['E_grade']} products

## How to Use These Files

1. **Review Analysis Results**: Open any `product_X.json` file to see the complete analysis pipeline from input ‚Üí normalization ‚Üí scoring ‚Üí explanation

2. **Verify Traceability**: Each file contains the full analysis trace showing how raw inputs were transformed into final scores

3. **Check Evidence**: All scoring factors reference authoritative sources listed in `sources_snapshot.json`

4. **Reproduce Results**: Use the barcode from any product file to look up the same analysis in the app

## Next Steps

1. Run the Streamlit app: `streamlit run app.py`
2. Use "Barcode Lookup" with any of the sample barcodes
3. Verify that cached results match these JSON files
4. Capture screenshots for documentation
"""
    
    with open(output_dir / "README.md", 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    # Final summary
    print("\nüéâ Sample Analysis Complete!")
    print("=" * 60)
    print(f"‚úÖ Analyzed: {len(products)} products")
    print(f"üíæ Database: {len(results)} records saved") 
    print(f"üìÅ Outputs: {output_dir}/ directory")
    print(f"üìä Average Score: {log_summary['average_score']:.1f}/100")
    print(f"üìö Sources: sources_snapshot.json")
    print(f"üìù Logs: food_rating.log")
    
    print("\nüöÄ Next Steps:")
    print("1. Run: streamlit run app.py")
    print("2. Test barcode lookup with: 123456789001, 123456789002, 123456789003")
    print("3. Capture screenshots for documentation")
    print("4. Review analysis traces in sample_outputs/")
    
    return results

def validate_outputs():
    """Validate that all expected outputs were created"""
    
    print("\nüîç Validating outputs...")
    
    required_files = [
        "products.db",
        "food_rating.log", 
        "sources_snapshot.json",
        "sample_outputs/product_1.json",
        "sample_outputs/product_2.json",
        "sample_outputs/product_3.json",
        "sample_outputs/run_summary.json",
        "sample_outputs/README.md"
    ]
    
    missing_files = []
    for file_path in required_files:
        if not Path(file_path).exists():
            missing_files.append(file_path)
    
    if missing_files:
        print("‚ùå Missing files:")
        for file_path in missing_files:
            print(f"   - {file_path}")
        return False
    else:
        print("‚úÖ All required files present")
        return True

def main():
    """Main execution function"""
    
    try:
        # Run the complete analysis
        results = run_complete_analysis()
        
        # Validate outputs
        if validate_outputs():
            print("\nüéØ Analysis pipeline completed successfully!")
            print("   Ready for demo and submission.")
        else:
            print("\n‚ö†Ô∏è  Some outputs missing. Check logs for errors.")
            return 1
            
        return 0
        
    except Exception as e:
        print(f"\n‚ùå Error during analysis: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())